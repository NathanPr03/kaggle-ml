{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Network Training\n",
    "The way the networks 'work out' the weight for each neuron is using a loss function and an optimiser\n",
    "\n",
    "## Loss Function\n",
    "This loss function compares what the model currently outputs (whether that be classification or regression) to some ground truth, This can use algos like MAE, MSE, etc.\n",
    "\n",
    "## Optimiser\n",
    "The optimiser - using the output of a loss function - alters the weights of the model to minimise the loss function. Most optimisers use Stochastic Gradient Descent (SGD). It works in steps, one step being:\n",
    "1. Sample some training data and run it through the network to make predictions.\n",
    "2. Measure the loss between the predictions and the true values.\n",
    "3. Finally, adjust the weights in a direction that makes the loss smaller."
   ],
   "id": "b0698980eeb8ad1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from learntools.deep_learning_intro.dltools import animate_sgd\n",
    "\n",
    "input_shape = 1000\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', # very good general self-learning SGD\n",
    "    loss='mae',\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    validation_data=(X, y),\n",
    "    batch_size=128,\n",
    "    epochs=200,\n",
    ")\n",
    "\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5. This can be changed to get a different view.\n",
    "history_df.loc[5:, ['loss']].plot();\n",
    "\n",
    "# We can add mroe params like thsi:\n",
    "learning_rate = 0.99\n",
    "batch_size = 32\n",
    "num_examples = 256\n",
    "\n",
    "#Realyl cool fucn that shows us how the SGD works\n",
    "animate_sgd(\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    "    num_examples=num_examples,\n",
    "    # You can also change these, if you like\n",
    "    steps=50, # total training steps (batches seen)\n",
    "    true_w=3.0, # the slope of the data\n",
    "    true_b=2.0, # the bias of the data\n",
    ")"
   ],
   "id": "a43cd608bc80a742"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent\n",
    "Okay, but how does SGD actually work?\n",
    "The 'gradient' is just a derivative of a loss function with respect to the models parameters.\n",
    "\n",
    "The derivative essentially just answers the question \"If I increase some weight `w`, does the loss go up or down?\". Obviously we want the loss to go down.\n",
    "\n",
    "But we only know if it will go up or down in local environment - not global - meaning the derivative cant just say \"Reduce some weight `w` by some value `x` to ge the best value for `w`\".\n",
    "\n",
    "It needs to be done in steps or epochs. changing w by some amount (set by the learning rate) and re-evaluation"
   ],
   "id": "6112c39ba081c374"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
