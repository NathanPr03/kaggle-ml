{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# XGBoost\n",
    "XGBoost standards for \"Extreme Gradient Boosting\". It builds on top of existing ensemble methods like random forests. The way it works is by building trees sequentially. The forest starts out with just one (bad and inaccurate)tree, that forest is then ran against a validation set, which is evaluate dby a loss function (like MAE) . The next tree is then built to correct the errors of the first using gradient descent (hence gradient boosting). This process is repeated, building more and more trees, each one trying to correct the errors of the previous trees. The final prediction is then made by averaging the predictions of all the trees."
   ],
   "id": "8ca9eb0bc424c882"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T22:05:37.420030Z",
     "start_time": "2025-09-18T22:05:35.878693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Load data using pipeline\n",
    "training_data_file_path = ('./data/4_housing_competition/train.csv')\n",
    "home_data = pd.read_csv(training_data_file_path)\n",
    "\n",
    "y = home_data.SalePrice\n",
    "\n",
    "feature_names = [\n",
    "    'MSSubClass',\n",
    "    'LotArea',\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'YearBuilt',\n",
    "    'YearRemodAdd',\n",
    "    '1stFlrSF',\n",
    "    '2ndFlrSF',\n",
    "    'LowQualFinSF',\n",
    "    'GrLivArea',\n",
    "    'FullBath',\n",
    "    'HalfBath',\n",
    "    'BedroomAbvGr',\n",
    "    'KitchenAbvGr',\n",
    "    'TotRmsAbvGrd',\n",
    "    'Fireplaces',\n",
    "    'WoodDeckSF',\n",
    "    'OpenPorchSF',\n",
    "    'EnclosedPorch',\n",
    "    '3SsnPorch',\n",
    "    'ScreenPorch',\n",
    "    'PoolArea',\n",
    "    'MiscVal',\n",
    "    'MoSold',\n",
    "    'YrSold'\n",
    "]\n",
    "\n",
    "X = home_data[feature_names]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# --- Custom thoughtful imputation for LotFrontage by MSSubClass ---\n",
    "if 'LotFrontage' in X_train.columns:\n",
    "    frontage_by_subclass = X_train.groupby('MSSubClass')['LotFrontage'].median()\n",
    "    # Impute in training set\n",
    "    X_train['LotFrontage'] = X_train.groupby('MSSubClass')['LotFrontage'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    # Impute in validation set using training medians\n",
    "    for subclass in X_valid['MSSubClass'].unique():\n",
    "        mask = X_valid['MSSubClass'] == subclass\n",
    "        if subclass in frontage_by_subclass:\n",
    "            fill_value = frontage_by_subclass[subclass]\n",
    "        else:\n",
    "            fill_value = X_train['LotFrontage'].median()\n",
    "        X_valid.loc[mask, 'LotFrontage'] = X_valid.loc[mask, 'LotFrontage'].fillna(fill_value)\n",
    "\n",
    "categorical_cols = [cname for cname in X_train.columns if\n",
    "                    X_train[cname].nunique() < 10 and\n",
    "                    X_train[cname].dtype == \"object\"]\n",
    "\n",
    "numerical_cols = [cname for cname in X_train.columns if\n",
    "                X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "good_label_cols = []\n",
    "for col in categorical_cols:\n",
    "    validation_values = set(X_valid[col])\n",
    "    training_values = set(X_train[col])\n",
    "    if validation_values.issubset(training_values):\n",
    "        good_label_cols.append(col)\n",
    "\n",
    "columns_to_keep = numerical_cols + good_label_cols\n",
    "\n",
    "X_train_filtered = X_train[columns_to_keep].copy()\n",
    "X_valid_filtered = X_valid[columns_to_keep].copy()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, good_label_cols)\n",
    "    ])\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', XGBRegressor( # Define model as XGBoost\n",
    "                                  n_estimators=1000, # Number of trees in forest\n",
    "                                  learning_rate=0.01, # Step size shrinkage used in update to prevents overfitting\n",
    "                                  n_jobs=4, # Parallelisation - number of CPU cores to use (4 cores in this case)\n",
    "                              ))])\n",
    "\n",
    "my_pipeline.fit(\n",
    "    X_train_filtered,\n",
    "    y_train,\n",
    ")\n",
    "\n",
    "preds = my_pipeline.predict(X_valid_filtered)\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_valid, preds)}\")\n",
    "\n",
    "# Retrain with all data\n",
    "my_pipeline.fit(X, y)\n",
    "\n",
    "test_data_file_path = ('./data/4_housing_competition/test.csv')\n",
    "test_data = pd.read_csv(test_data_file_path)\n",
    "X_test = test_data[feature_names]\n",
    "test_predictions = my_pipeline.predict(X_test)"
   ],
   "id": "4d666143b97172b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 16674.673828125\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T22:05:39.205518Z",
     "start_time": "2025-09-18T22:05:39.199568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_csv_from_predictions():\n",
    "    output = pd.DataFrame({'Id': test_data.Id,\n",
    "                           'SalePrice': test_predictions})\n",
    "\n",
    "    output.to_csv('/Users/natedev/repos/machine-learning/data/4_housing_competition/submission.csv', index=False)\n",
    "    print(\"Your submission was successfully saved!\")\n",
    "\n",
    "generate_csv_from_predictions()"
   ],
   "id": "1cd93af3e7f2d107",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0536e080a534316"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
