{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Imputation\n",
    "How do we handle rows with missing data? We have some options:\n",
    "- Drop them\n",
    "- Impute them (fill them in with some value)\n",
    "    - Mean\n",
    "    - Median\n",
    "    - Misc\n",
    "\n",
    "This will look like:"
   ],
   "id": "fea741492d18ba1a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-13T22:39:50.581867Z",
     "start_time": "2025-09-13T22:39:49.466962Z"
    }
   },
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('./data/4_housing_competition/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./data/4_housing_competition/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Break off validation set from training data`\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "# ----------- DROPPING COLUMNS -----------\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Fill in the lines below: drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "\n",
    "# -------- MEAN IMPUTATION -----------\n",
    "imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 36)\n",
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n",
      "MAE (Drop columns with missing values):\n",
      "17837.82570776256\n",
      "MAE (Imputation):\n",
      "18062.894611872147\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From this we can see that imputation actually performs worse than dropping the columns. This is likely because the columns we dropped were not very informative to begin with. In a real scenario, you would want to try and keep as much data as possible, so imputation is often the better choice. However, it's always good to try both and see which works better for your specific dataset.\n",
    "\n",
    "Lets see if we can get smarter with our imputation:"
   ],
   "id": "d6c84e8f0d6ee6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T22:39:53.145236Z",
     "start_time": "2025-09-13T22:39:52.576900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "frontage_by_subclass = X_train.groupby('MSSubClass')['LotFrontage'].median()\n",
    "\n",
    "# Do thoughtful imputation, we can use the median LotFrontage for each MSSubClass (MSSubClass is a 'house style' to fill in missing values\n",
    "X_train['LotFrontage'] = X_train.groupby('MSSubClass')['LotFrontage'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Apply to validation set using training data medians\n",
    "for subclass in X_valid['MSSubClass'].unique():\n",
    "    mask = X_valid['MSSubClass'] == subclass\n",
    "    if subclass in frontage_by_subclass:\n",
    "        fill_value = frontage_by_subclass[subclass]\n",
    "    else:\n",
    "        # If subclass not in training data, use overall training median\n",
    "        fill_value = X_train['LotFrontage'].median()\n",
    "\n",
    "    X_valid.loc[mask, 'LotFrontage'] = X_valid.loc[mask, 'LotFrontage'].fillna(fill_value)\n",
    "\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "final_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "final_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(final_X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "preds_valid = model.predict(final_X_valid)\n",
    "print(\"MAE (Your approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))"
   ],
   "id": "de8a25b2af333982",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Your approach):\n",
      "17915.223036529682\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And this performs better..\n",
    "\n",
    "So the general idea is try first:\n",
    "- Drop columns with missing values\n",
    "- Mean/Median imputation\n",
    "- More thoughtful imputation\n",
    "\n",
    "And then compare. YMMV based on your dataset."
   ],
   "id": "a15953d260a70aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ced7ef02d1ae434"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
