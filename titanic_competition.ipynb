{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Titanic Kaggle competition\n",
    "https://www.kaggle.com/competitions/titanic/overview"
   ],
   "id": "75b3601c2214cd70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T18:35:34.197878Z",
     "start_time": "2025-09-21T18:35:34.067878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic/train.csv'\n",
    "\n",
    "def extract_title(name: str) -> str:\n",
    "    import re\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Group similar titles together\n",
    "        if title in ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n",
    "            return 'Rare'\n",
    "        elif title in ['Mlle', 'Ms']: # French\n",
    "            return 'Miss'\n",
    "        elif title == 'Mme': # French\n",
    "            return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic/train.csv'\n",
    "\n",
    "def extract_title(name: str) -> str:\n",
    "    import re\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Group similar titles together\n",
    "        if title in ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n",
    "            return 'Rare'\n",
    "        elif title in ['Mlle', 'Ms']: # French\n",
    "            return 'Miss'\n",
    "        elif title == 'Mme': # French\n",
    "            return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "class FareBinTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to create fare bins and handle missing values\"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=4, strategy='quantile'):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "        self.fare_bins_ = None\n",
    "        self.fare_bin_labels_ = ['Low', 'Medium', 'High', 'Very_High']\n",
    "        self.class_medians_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learn the fare bins and class-based medians from training data\"\"\"\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # First, learn class-based medians for imputing missing fares\n",
    "        self.class_medians_ = X_copy.groupby('Pclass')['Fare'].median().to_dict()\n",
    "\n",
    "        # Fill missing fares using class medians\n",
    "        X_copy['Fare'] = X_copy.apply(\n",
    "            lambda row: self.class_medians_[row['Pclass']] if pd.isna(row['Fare']) else row['Fare'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Learn fare bins\n",
    "        if self.strategy == 'quantile':\n",
    "            # Use quantile-based bins for balanced groups\n",
    "            self.fare_bins_ = pd.qcut(X_copy['Fare'], q=self.n_bins, retbins=True, duplicates='drop')[1]\n",
    "        else:\n",
    "            # Use equal-width bins\n",
    "            self.fare_bins_ = pd.cut(X_copy['Fare'], bins=self.n_bins, retbins=True)[1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply fare binning to new data\"\"\"\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # Impute missing fares using learned class medians\n",
    "        X_copy['Fare'] = X_copy.apply(\n",
    "            lambda row: self.class_medians_.get(row['Pclass'], X_copy['Fare'].median())\n",
    "            if pd.isna(row['Fare']) else row['Fare'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Apply learned fare bins\n",
    "        X_copy['FareBin'] = pd.cut(\n",
    "            X_copy['Fare'],\n",
    "            bins=self.fare_bins_,\n",
    "            labels=self.fare_bin_labels_[:len(self.fare_bins_)-1],\n",
    "            include_lowest=True\n",
    "        )\n",
    "\n",
    "        # Handle any values outside learned range (shouldn't happen but safety first)\n",
    "        X_copy['FareBin'] = X_copy['FareBin'].fillna('Medium')\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class TitleExtractorTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to extract titles from names\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Title'] = X_copy['Name'].apply(extract_title)\n",
    "        return X_copy\n",
    "\n",
    "def retrieve_sanitised_data_frame(file_path) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    titanic_passenger_data = pd.read_csv(file_path).replace(['', ' ', '  '], np.nan)\n",
    "\n",
    "    y = titanic_passenger_data.get('Survived', None)\n",
    "\n",
    "    # Convert strings to numeric where possible\n",
    "    # titanic_passenger_data = titanic_passenger_data.apply(pd.to_numeric, errors='ignore')\n",
    "    # titanic_passenger_data['SibSp'] = titanic_passenger_data['SibSp'].astype('float64')\n",
    "    # titanic_passenger_data['Parch'] = titanic_passenger_data['Parch'].astype('float64')\n",
    "    # titanic_passenger_data['Age'] = titanic_passenger_data['Age'].astype('float64')\n",
    "    #\n",
    "    # # Basic feature engineering (keep this outside pipeline for simplicity)\n",
    "    # titanic_passenger_data['FamilySize'] = titanic_passenger_data['SibSp'] + titanic_passenger_data['Parch']\n",
    "    # titanic_passenger_data['IsAlone'] = (titanic_passenger_data['FamilySize'] == 0).astype(int)\n",
    "\n",
    "    feature_names = titanic_passenger_data.columns\n",
    "    X = titanic_passenger_data[feature_names].drop(\n",
    "        ['PassengerId', 'Survived', 'Cabin'],\n",
    "        axis=1,\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    X = X.drop(['Ticket'], axis=1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = retrieve_sanitised_data_frame(file_path)\n",
    "print(X.columns)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    ('title_extractor', TitleExtractorTransformer()),\n",
    "    ('fare_binner', FareBinTransformer(n_bins=4, strategy='quantile')),\n",
    "])\n",
    "\n",
    "# Fit feature pipeline on training data to see resulting columns\n",
    "# X_train_engineered = feature_pipeline.fit_transform(X_train)\n",
    "# print(f\"Columns after feature engineering: {list(X_train_engineered.columns)}\")\n",
    "# print(f\"Data types: {X_train_engineered.dtypes}\")\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('median_imputer', median_imputer, ['Age']),\n",
    "        ('zero_imputer', zero_imputer, ['SibSp']),\n",
    "        ('categorical', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # There are some missing Embarked rows\n",
    "             # List all categories explicitly. When we create fold this prevents unknown category issues\n",
    "            ('encoder', OneHotEncoder(categories=[\n",
    "                ['female', 'male'], # Sex\n",
    "                ['C', 'Q', 'S'], # Embarked\n",
    "                # ['Mr', 'Mrs', 'Miss', 'Master', 'Rare'], # Title\n",
    "                # ['Low', 'Medium', 'High', 'Very_High'] # FareBin\n",
    "            ]))\n",
    "        ]), ['Sex', 'Embarked', ]),\n",
    "        # ('fare_imputer', SimpleImputer(strategy='median'), ['Fare']),\n",
    "        ('passthrough', 'passthrough', ['Pclass', 'Parch']),\n",
    "    ])\n",
    "\n",
    "xgbModel = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    n_jobs=4, # Parallelisation - number of CPU cores to use (4 cores in this case)\n",
    "    random_state=1\n",
    ")\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    # ('feature_engineering', feature_pipeline),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgbModel)\n",
    "])\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, preds)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_valid, preds))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_valid, preds))"
   ],
   "id": "3870edede0a2a56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')\n",
      "Accuracy: 0.7847533632286996\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83       128\n",
      "           1       0.87      0.58      0.70        95\n",
      "\n",
      "    accuracy                           0.78       223\n",
      "   macro avg       0.81      0.76      0.76       223\n",
      "weighted avg       0.80      0.78      0.77       223\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[120   8]\n",
      " [ 40  55]]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T18:26:01.203181Z",
     "start_time": "2025-09-21T18:26:01.196181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Importance Analysis\n",
    "feature_names = my_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "importances = my_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f'\\n=== TOP 10 FEATURE IMPORTANCES ===')\n",
    "print(feature_importance_df.head(10))"
   ],
   "id": "cc0f7fa9f9114de9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 10 FEATURE IMPORTANCES ===\n",
      "                      feature  importance\n",
      "7       categorical__Title_Mr    0.370191\n",
      "17        passthrough__Pclass    0.081837\n",
      "11    categorical__Title_Rare    0.074934\n",
      "19    passthrough__FamilySize    0.070243\n",
      "2     categorical__Sex_female    0.041455\n",
      "6     categorical__Embarked_S    0.038860\n",
      "16         fare_imputer__Fare    0.035828\n",
      "12   categorical__FareBin_Low    0.031277\n",
      "10  categorical__Title_Master    0.030071\n",
      "0         median_imputer__Age    0.026786\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T18:32:23.471911Z",
     "start_time": "2025-09-21T18:32:15.923808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def print_best_params_as_code(grid_search, model_name=\"best_model\"):\n",
    "    \"\"\"Print the best parameters as copy-pastable Python code\"\"\"\n",
    "    params = grid_search.best_params_\n",
    "\n",
    "    print(f\"\\n# Best parameters found:\")\n",
    "    print(f\"{model_name} = XGBClassifier(\")\n",
    "\n",
    "    # Extract model parameters (remove 'model__' prefix)\n",
    "    model_params = {}\n",
    "    for key, value in params.items():\n",
    "        if key.startswith('model__'):\n",
    "            clean_key = key.replace('model__', '')\n",
    "            model_params[clean_key] = value\n",
    "\n",
    "    # Print each parameter\n",
    "    for key, value in model_params.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"    {key}='{value}',\")\n",
    "        else:\n",
    "            print(f\"    {key}={value},\")\n",
    "\n",
    "    print(\")\")\n",
    "\n",
    "print(f'\\n=== HYPERPARAMETER TUNING ===')\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Use the existing pipeline - just need to create a fresh copy for grid search\n",
    "from sklearn.base import clone\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    clone(my_pipeline),  # Clone the existing pipeline to avoid fitting conflicts\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print_best_params_as_code(grid_search)\n",
    "\n",
    "# Evaluate best model\n",
    "best_preds = grid_search.predict(X_valid)\n",
    "best_accuracy = accuracy_score(y_valid, best_preds)\n",
    "best_f1 = f1_score(y_valid, best_preds, average='macro')\n",
    "\n",
    "print(f'\\n=== BEST MODEL PERFORMANCE ===')\n",
    "print(f'Best CV Score: {grid_search.best_score_:.4f}')\n",
    "print(f'Validation Accuracy: {best_accuracy:.4f}')\n",
    "print(f'Validation F1: {best_f1:.4f}')"
   ],
   "id": "a375f935442dd9eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Starting grid search...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.8}\n",
      "\n",
      "# Best parameters found:\n",
      "best_model = XGBClassifier(\n",
      "    learning_rate=0.01,\n",
      "    max_depth=3,\n",
      "    n_estimators=300,\n",
      "    subsample=0.8,\n",
      ")\n",
      "\n",
      "=== BEST MODEL PERFORMANCE ===\n",
      "Best CV Score: 0.8413\n",
      "Validation Accuracy: 0.7892\n",
      "Validation F1: 0.7716\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T18:35:41.769313Z",
     "start_time": "2025-09-21T18:35:41.678279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare submission\n",
    "\n",
    "# Retrain model fully\n",
    "my_pipeline.fit(X, y)\n",
    "\n",
    "test_data_file_path = './data/titanic/test.csv'\n",
    "X_test, _ = retrieve_sanitised_data_frame(test_data_file_path)\n",
    "\n",
    "predictions_on_test_data = my_pipeline.predict(X_test)\n",
    "\n",
    "# For PassengerId, read it separately since we drop it in retrieve_sanitised_data_frame()\n",
    "\n",
    "test_data_raw = pd.read_csv(test_data_file_path)\n",
    "output = pd.DataFrame({'PassengerId': test_data_raw.PassengerId, 'Survived': predictions_on_test_data})\n",
    "output.to_csv('./data/titanic/submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "id": "d912c10e485be044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Steps\n",
    "\n",
    "- [x] Load data\n",
    "- [x] Select columns we want\n",
    "- [x] Temporarily exclude cabin and ticket\n",
    "- [ ] Impute age\n",
    "    - [x] Mean\n",
    "    - [ ] Smarter - can we work this out from name? Ticket price? Location\n",
    "- [x] One-hot encode sex\n",
    "- [ ] Get smart with ticket and cabin\n",
    "    - [x] Cabin tuning <-- Discovered its better to just drop the cabin! As its very sparsely populated in test set\n",
    "- [x] Create decision tree\n",
    "    - [x] Tune decision tree\n",
    "- [x] XGBoost\n",
    "- [x] Try cross fold validation <-- Not doing this as we already have a test set"
   ],
   "id": "8c3c0558763c5511"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
