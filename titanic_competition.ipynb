{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Titanic Kaggle competition\n",
    "https://www.kaggle.com/competitions/titanic/overview"
   ],
   "id": "75b3601c2214cd70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:02:12.893076Z",
     "start_time": "2025-09-25T13:02:12.788378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from numpy.f2py.symbolic import as_eq\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic/train.csv'\n",
    "\n",
    "def extract_title(name: str) -> str:\n",
    "    import re\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Group similar titles together\n",
    "        if title in ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n",
    "            return 'Rare'\n",
    "        elif title in ['Mlle', 'Ms']: # French\n",
    "            return 'Miss'\n",
    "        elif title == 'Mme': # French\n",
    "            return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic/train.csv'\n",
    "\n",
    "def extract_title(name: str) -> str:\n",
    "    import re\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Group similar titles together\n",
    "        if title in ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n",
    "            return 'Rare'\n",
    "        elif title in ['Mlle', 'Ms']: # French\n",
    "            return 'Miss'\n",
    "        elif title == 'Mme': # French\n",
    "            return 'Mrs'\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "class FareBinTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to create fare bins and handle missing values\"\"\"\n",
    "\n",
    "    def __init__(self, n_bins=4, strategy='quantile'):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "        self.fare_bins_ = None\n",
    "        self.fare_bin_labels_ = ['Low', 'Medium', 'High', 'Very_High']\n",
    "        self.class_medians_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learn the fare bins and class-based medians from training data\"\"\"\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # First, learn class-based medians for imputing missing fares\n",
    "        self.class_medians_ = X_copy.groupby('Pclass')['Fare'].median().to_dict()\n",
    "\n",
    "        # Fill missing fares using class medians\n",
    "        X_copy['Fare'] = X_copy.apply(\n",
    "            lambda row: self.class_medians_[row['Pclass']] if pd.isna(row['Fare']) else row['Fare'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Learn fare bins\n",
    "        if self.strategy == 'quantile':\n",
    "            # Use quantile-based bins for balanced groups\n",
    "            self.fare_bins_ = pd.qcut(X_copy['Fare'], q=self.n_bins, retbins=True, duplicates='drop')[1]\n",
    "        else:\n",
    "            # Use equal-width bins\n",
    "            self.fare_bins_ = pd.cut(X_copy['Fare'], bins=self.n_bins, retbins=True)[1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply fare binning to new data\"\"\"\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # Impute missing fares using learned class medians\n",
    "        X_copy['Fare'] = X_copy.apply(\n",
    "            lambda row: self.class_medians_.get(row['Pclass'], X_copy['Fare'].median())\n",
    "            if pd.isna(row['Fare']) else row['Fare'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Apply learned fare bins\n",
    "        X_copy['FareBin'] = pd.cut(\n",
    "            X_copy['Fare'],\n",
    "            bins=self.fare_bins_,\n",
    "            labels=self.fare_bin_labels_[:len(self.fare_bins_)-1],\n",
    "            include_lowest=True\n",
    "        )\n",
    "\n",
    "        # Handle any values outside learned range (shouldn't happen but safety first)\n",
    "        X_copy['FareBin'] = X_copy['FareBin'].fillna('Medium')\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class TitleExtractorTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to extract titles from names\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['Title'] = X_copy['Name'].apply(extract_title)\n",
    "        return X_copy\n",
    "\n",
    "def extract_ticket_nuber(x):\n",
    "    return x.split(' ')[-1]\n",
    "\n",
    "def extract_ticket_item(x):\n",
    "        items = x.split(\" \")\n",
    "        if len(items) == 1:\n",
    "            return \"NONE\"\n",
    "        return \"_\".join(items[0:-1])\n",
    "\n",
    "def retrieve_sanitised_data_frame(file_path) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    titanic_passenger_data = pd.read_csv(file_path).replace(['', ' ', '  '], np.nan)\n",
    "\n",
    "    y = titanic_passenger_data.get('Survived', None)\n",
    "\n",
    "    # Convert strings to numeric where possible\n",
    "    # titanic_passenger_data = titanic_passenger_data.apply(pd.to_numeric, errors='ignore')\n",
    "    # titanic_passenger_data['SibSp'] = titanic_passenger_data['SibSp'].astype('float64')\n",
    "    # titanic_passenger_data['Parch'] = titanic_passenger_data['Parch'].astype('float64')\n",
    "    # titanic_passenger_data['Age'] = titanic_passenger_data['Age'].astype('float64')\n",
    "    #\n",
    "    # # Basic feature engineering (keep this outside pipeline for simplicity)\n",
    "    # titanic_passenger_data['FamilySize'] = titanic_passenger_data['SibSp'] + titanic_passenger_data['Parch']\n",
    "    # titanic_passenger_data['IsAlone'] = (titanic_passenger_data['FamilySize'] == 0).astype(int)\n",
    "\n",
    "    # titanic_passenger_data['TicketNumber'] = titanic_passenger_data['Ticket'].apply(extract_ticket_nuber)\n",
    "    # titanic_passenger_data['TicketItem'] = titanic_passenger_data['Ticket'].apply(extract_ticket_item)\n",
    "\n",
    "    feature_names = titanic_passenger_data.columns\n",
    "    X = titanic_passenger_data[feature_names].drop(\n",
    "        ['PassengerId', 'Survived', 'Cabin'],\n",
    "        axis=1,\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    X = X.drop(['Ticket'], axis=1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = retrieve_sanitised_data_frame(file_path)\n",
    "print(X.columns)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    ('title_extractor', TitleExtractorTransformer()),\n",
    "    ('fare_binner', FareBinTransformer(n_bins=4, strategy='quantile')),\n",
    "])\n",
    "\n",
    "# Fit feature pipeline on training data to see resulting columns\n",
    "# X_train_engineered = feature_pipeline.fit_transform(X_train)\n",
    "# print(f\"Columns after feature engineering: {list(X_train_engineered.columns)}\")\n",
    "# print(f\"Data types: {X_train_engineered.dtypes}\")\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('median_imputer', median_imputer, ['Age']),\n",
    "        ('zero_imputer', zero_imputer, ['SibSp']),\n",
    "        ('categorical', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # There are some missing Embarked rows\n",
    "             # List all categories explicitly. When we create fold this prevents unknown category issues\n",
    "            ('encoder', OneHotEncoder(categories=[\n",
    "                ['female', 'male'], # Sex\n",
    "                ['C', 'Q', 'S'], # Embarked\n",
    "                # ['Mr', 'Mrs', 'Miss', 'Master', 'Rare'], # Title\n",
    "                # ['Low', 'Medium', 'High', 'Very_High'] # FareBin\n",
    "            ]))\n",
    "        ]), ['Sex', 'Embarked',\n",
    "             # 'Title', 'FareBin'\n",
    "             ]),\n",
    "        ('fare_imputer', SimpleImputer(strategy='median'), ['Fare']),\n",
    "        ('passthrough', 'passthrough', ['Pclass', 'Parch']),\n",
    "    ])\n",
    "\n",
    "xgbModel = XGBClassifier(\n",
    "   learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    subsample=0.8,\n",
    "    n_jobs=4, # Parallelisation - number of CPU cores to use (4 cores in this case)\n",
    "    random_state=1\n",
    ")\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    # ('feature_engineering', feature_pipeline),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgbModel)\n",
    "])\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, preds)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_valid, preds))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_valid, preds))"
   ],
   "id": "3870edede0a2a56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')\n",
      "Accuracy: 0.7937219730941704\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.95      0.84       128\n",
      "           1       0.90      0.58      0.71        95\n",
      "\n",
      "    accuracy                           0.79       223\n",
      "   macro avg       0.83      0.77      0.77       223\n",
      "weighted avg       0.82      0.79      0.78       223\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[122   6]\n",
      " [ 40  55]]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T12:26:49.683344Z",
     "start_time": "2025-09-25T12:26:49.661598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Importance Analysis\n",
    "feature_names = my_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "importances = my_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f'\\n=== TOP 10 FEATURE IMPORTANCES ===')\n",
    "print(feature_importance_df.head(10))"
   ],
   "id": "cc0f7fa9f9114de9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP 10 FEATURE IMPORTANCES ===\n",
      "                   feature  importance\n",
      "2  categorical__Sex_female    0.493289\n",
      "7      passthrough__Pclass    0.129119\n",
      "6  categorical__Embarked_S    0.102587\n",
      "1      zero_imputer__SibSp    0.065006\n",
      "0      median_imputer__Age    0.061901\n",
      "4  categorical__Embarked_C    0.060091\n",
      "5  categorical__Embarked_Q    0.056258\n",
      "8       passthrough__Parch    0.031750\n",
      "3    categorical__Sex_male    0.000000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:01:48.431682Z",
     "start_time": "2025-09-25T13:01:45.203767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FIXED HYPERPARAMETER TUNING - PRESERVING YOUR SPLIT\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def print_best_params_as_code(grid_search, model_name=\"best_model\"):\n",
    "    \"\"\"Print the best parameters as copy-pastable Python code\"\"\"\n",
    "    params = grid_search.best_params_\n",
    "\n",
    "    print(f\"\\n# Best parameters found:\")\n",
    "    print(f\"{model_name} = XGBClassifier(\")\n",
    "\n",
    "    # Extract model parameters (remove 'model__' prefix)\n",
    "    model_params = {}\n",
    "    for key, value in params.items():\n",
    "        if key.startswith('model__'):\n",
    "            clean_key = key.replace('model__', '')\n",
    "            model_params[clean_key] = value\n",
    "\n",
    "    # Print each parameter\n",
    "    for key, value in model_params.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"    {key}='{value}',\")\n",
    "        else:\n",
    "            print(f\"    {key}={value},\")\n",
    "\n",
    "    print(\")\")\n",
    "\n",
    "print(f'\\n=== HYPERPARAMETER TUNING ===')\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'model__subsample': [0.8, 0.9],\n",
    "    'model__reg_alpha': [0, 0.1],\n",
    "    'model__reg_lambda': [0.5, 1.0]\n",
    "}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    clone(my_pipeline),\n",
    "    param_grid,\n",
    "    cv=cv_strategy,      # This will split X_train into 5 folds internally\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "\n",
    "# ONLY use X_train, y_train - GridSearch will do CV internally on this\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print_best_params_as_code(grid_search)\n",
    "\n",
    "# NOW use X_valid for monitoring (don't change hyperparameters based on this!)\n",
    "best_preds = grid_search.predict(X_valid)\n",
    "best_accuracy = accuracy_score(y_valid, best_preds)\n",
    "best_f1 = f1_score(y_valid, best_preds, average='macro')\n",
    "\n",
    "print(f'\\n=== BEST MODEL PERFORMANCE ===')\n",
    "print(f'Best CV Score: {grid_search.best_score_:.4f}')\n",
    "print(f'Validation Accuracy: {best_accuracy:.4f}')\n",
    "print(f'Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Check for overfitting\n",
    "cv_val_gap = abs(grid_search.best_score_ - best_accuracy)\n",
    "print(f'CV-Validation Gap: {cv_val_gap:.4f}')\n",
    "\n",
    "if cv_val_gap > 0.05:\n",
    "    print(\"⚠️  WARNING: Large gap suggests overfitting!\")\n",
    "else:\n",
    "    print(\"✅ Good generalization\")"
   ],
   "id": "a375f935442dd9eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Starting grid search...\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best parameters: {'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__n_estimators': 100, 'model__reg_alpha': 0.1, 'model__reg_lambda': 0.5, 'model__subsample': 0.8}\n",
      "\n",
      "# Best parameters found:\n",
      "best_model = XGBClassifier(\n",
      "    learning_rate=0.05,\n",
      "    max_depth=5,\n",
      "    n_estimators=100,\n",
      "    reg_alpha=0.1,\n",
      "    reg_lambda=0.5,\n",
      "    subsample=0.8,\n",
      ")\n",
      "\n",
      "=== BEST MODEL PERFORMANCE ===\n",
      "Best CV Score: 0.8399\n",
      "Validation Accuracy: 0.7937\n",
      "Validation F1: 0.7733\n",
      "CV-Validation Gap: 0.0462\n",
      "✅ Good generalization\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:02:24.289484Z",
     "start_time": "2025-09-25T13:02:24.210334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare submission\n",
    "\n",
    "# Retrain model fully\n",
    "my_pipeline.fit(X, y)\n",
    "\n",
    "test_data_file_path = './data/titanic/test.csv'\n",
    "X_test, _ = retrieve_sanitised_data_frame(test_data_file_path)\n",
    "\n",
    "predictions_on_test_data = my_pipeline.predict(X_test)\n",
    "\n",
    "# For PassengerId, read it separately since we drop it in retrieve_sanitised_data_frame()\n",
    "\n",
    "test_data_raw = pd.read_csv(test_data_file_path)\n",
    "output = pd.DataFrame({'PassengerId': test_data_raw.PassengerId, 'Survived': predictions_on_test_data})\n",
    "output.to_csv('./data/titanic/submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "id": "d912c10e485be044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Steps\n",
    "\n",
    "- [x] Load data\n",
    "- [x] Select columns we want\n",
    "- [x] Temporarily exclude cabin and ticket\n",
    "- [ ] Impute age\n",
    "    - [x] Mean\n",
    "    - [ ] Smarter - can we work this out from name? Ticket price? Location\n",
    "- [x] One-hot encode sex\n",
    "- [ ] Get smart with ticket and cabin\n",
    "    - [x] Cabin tuning <-- Discovered its better to just drop the cabin! As its very sparsely populated in test set\n",
    "- [x] Create decision tree\n",
    "    - [x] Tune decision tree\n",
    "- [x] XGBoost\n",
    "- [x] Try cross fold validation <-- Not doing this as we already have a test set"
   ],
   "id": "8c3c0558763c5511"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
