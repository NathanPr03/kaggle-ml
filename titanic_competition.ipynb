{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Titanic Kaggle competition\n",
    "https://www.kaggle.com/competitions/titanic/overview"
   ],
   "id": "75b3601c2214cd70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T13:28:26.790834Z",
     "start_time": "2025-09-21T13:28:26.672821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.signal import qspline1d\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './data/titanic/train.csv'\n",
    "\n",
    "def retrieve_sanitised_data_frame(file_path) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    titanic_passenger_data = pd.read_csv(file_path).replace(['', ' ', '  '], np.nan)\n",
    "\n",
    "    y = titanic_passenger_data.get('Survived', None) # handle case where it doesnt exist (test_data)\n",
    "\n",
    "    feature_names = titanic_passenger_data.columns\n",
    "    X = titanic_passenger_data[feature_names].drop(\n",
    "        ['Name', 'PassengerId', 'Survived', 'Cabin'],\n",
    "        axis=1,\n",
    "        errors='ignore' # If we fail to drop survived we dont care\n",
    "    ) # Names probs not useful\n",
    "\n",
    "    X = X.drop(['Ticket'], axis=1) # TODO: This is temporary just to get up and running\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = retrieve_sanitised_data_frame(file_path)\n",
    "print(X.columns)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X[['Sex', 'Embarked']].isnull().sum())\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('mean_imputer', mean_imputer, ['Age']),\n",
    "        ('median_imputer', median_imputer, ['Age']),\n",
    "        ('zero_imputer', zero_imputer, ['SibSp']),\n",
    "        ('categorical', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # There are some missing Embarked rows\n",
    "             # List all categories explicitly. When we create fold this prevents unknown category issues\n",
    "            ('encoder', OneHotEncoder(categories=[['female', 'male'], ['C', 'Q', 'S']]))\n",
    "        ]), ['Sex', 'Embarked']),\n",
    "    ])\n",
    "\n",
    "xgbModel = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    n_jobs=4, # Parallelisation - number of CPU cores to use (4 cores in this case)\n",
    ")\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', xgbModel)\n",
    "                             ])\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, preds)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_valid, preds))\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_valid, preds))"
   ],
   "id": "3870edede0a2a56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], dtype='object')\n",
      "Sex         0\n",
      "Embarked    2\n",
      "dtype: int64\n",
      "Accuracy: 0.7847533632286996\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       128\n",
      "           1       0.81      0.65      0.72        95\n",
      "\n",
      "    accuracy                           0.78       223\n",
      "   macro avg       0.79      0.77      0.77       223\n",
      "weighted avg       0.79      0.78      0.78       223\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[113  15]\n",
      " [ 33  62]]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T13:28:13.848024Z",
     "start_time": "2025-09-21T13:28:12.333143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Grid Search for Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def print_best_params_as_code(grid_search, model_name=\"best_model\"):\n",
    "    \"\"\"Print the best parameters as copy-pastable Python code\"\"\"\n",
    "    params = grid_search.best_params_\n",
    "\n",
    "    print(f\"\\n# Best parameters found:\")\n",
    "    print(f\"{model_name} = XGBClassifier(\")\n",
    "\n",
    "    # Extract model parameters (remove 'model__' prefix)\n",
    "    model_params = {}\n",
    "    for key, value in params.items():\n",
    "        if key.startswith('model__'):\n",
    "            clean_key = key.replace('model__', '')\n",
    "            model_params[clean_key] = value\n",
    "\n",
    "    # Print each parameter\n",
    "    for key, value in model_params.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"    {key}='{value}',\")\n",
    "        else:\n",
    "            print(f\"    {key}={value},\")\n",
    "\n",
    "    print(\")\")\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5, 6],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Fit the preprocessor with ALL training data. That way when we split it into folds below, we dont end up with unseen categories\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Then use in GridSearch\n",
    "grid_search = GridSearchCV(\n",
    "    Pipeline([('preprocessor', preprocessor), ('model', xgbModel)]),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1 # Progress info\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print_best_params_as_code(grid_search)"
   ],
   "id": "a375f935442dd9eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.8}\n",
      "\n",
      "# Best parameters found:\n",
      "best_model = XGBClassifier(\n",
      "    learning_rate=0.01,\n",
      "    max_depth=3,\n",
      "    n_estimators=300,\n",
      "    subsample=0.8,\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T20:50:54.253217Z",
     "start_time": "2025-09-19T20:50:54.046500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare submission\n",
    "\n",
    "# Retrain model fully\n",
    "my_pipeline.fit(X, y)\n",
    "\n",
    "test_data_file_path = './data/titanic/test.csv'\n",
    "test_data = pd.read_csv(test_data_file_path).replace(['', ' ', '  '], np.nan)\n",
    "\n",
    "predictions_on_test_data = my_pipeline.predict(test_data)\n",
    "\n",
    "# Save to CSV\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_on_test_data})\n",
    "output.to_csv('./data/titanic/submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")\n"
   ],
   "id": "d912c10e485be044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Steps\n",
    "\n",
    "- [x] Load data\n",
    "- [x] Select columns we want\n",
    "- [x] Temporarily exclude cabin and ticket\n",
    "- [ ] Impute age\n",
    "    - [x] Mean\n",
    "    - [ ] Smarter - can we work this out from name? Ticket price? Location\n",
    "- [x] One-hot encode sex\n",
    "- [ ] Get smart with ticket and cabin\n",
    "    - [x] Cabin tuning <-- Discovered its better to just drop the cabin! As its very sparsely populated in test set\n",
    "- [x] Create decision tree\n",
    "    - [x] Tune decision tree\n",
    "- [x] XGBoost\n",
    "- [x] Try cross fold validation <-- Not doing this as we already have a test set"
   ],
   "id": "8c3c0558763c5511"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
